{"instruction": "Example 5.13:8 (a) What does 95% confident mean in this context? (b) Do you think the confidence interval is still valid for the opinions of New Yorkers today? 8(a) If we took many such samples and computed a 95% confidence interval for each, then about 95% of those intervals would contain the actual proportion of New York adults who supported a quarantine for anyone who has come into contact with an Ebola patient. (b) Not necessarily. The poll was taken at a time where there was a huge public safety concern. Now that people have had some time to step back, they may have changed their opinions. We would need to run a new poll if we wanted to get an estimate of the current proportion of New York adults who would support such a quarantine period.", "response": "186 CHAPTER 5. FOUNDATIONS FOR INFERENCE GUIDED PRACTICE 5.15 In the Pew Research poll about solar energy, they also inquired about other forms of energy, and"}
{"instruction": "EXAMPLE 5.21 Explain why we cannot conclude that college-educated adults simply guessed on the infant vaccina- tion question. While we failed to reject H0, that does not necessarily mean the null hypothesis is true. Perhaps there was an actual difference, but we were not able to detect it with the relatively small sample of 50. DOUBLE NEGATIVES CAN SOMETIMES BE USED IN STATISTICS In many statistical explanations, we use double negatives. For instance, we might say that the null hypothesis is not implausible or we failed to reject the null hypothesis. Double negatives are used to communicate that while we are not rejecting a position, we are also not saying it is correct. 16Arguably this method is slightly imprecise. As we’ll see in a few pages, the standard error is often computed slightly differently in the context of a hypothesis test for a proportion.", "response": "192 CHAPTER 5. FOUNDATIONS FOR INFERENCE GUIDED PRACTICE 5.22 Let’s move onto a second question posed by the Roslings: There are 2 billion children in the world today aged 0-15 years old, how many children will there be in year 2100 according to the United Nations? a. 4 billion. b. 3 billion. c. 2 billion. Set up appropriate hypotheses to evaluate whether college-educated adults are better than random guessing on this question. Also, see if you can guess the correct answer before checking the answer in the footnote!17 GUIDED PRACTICE 5.23 This time we took a larger sample of 228 college-educated adults, 34 (14.9%) selected the correct answer to the question in Guided Practice 5.22: 2 billion. Can we model the sample proportion using a normal distribution and construct a confidence interval?18"}
{"instruction": "EXAMPLE 5.38 Why can’t we simply run a one-sided test that goes in the direction of the data? We’ve been building a careful framework that controls for the Type 1 Error, which is the significance level α in a hypothesis test. We’ll use the α = 0.05 below to keep things simple. Imagine we could pick the one-sided test after we saw the data. What will go wrong? • If ˆp is smaller than the null value, then a one-sided test where p < p0 would mean that any observation in the lower 5% tail of the null distribution would lead to us rejecting H0. • If ˆp is larger than the null value, then a one-sided test where p > p0 would mean that any observation in the upper 5% tail of the null distribution would lead to us rejecting H0. Then if H0 were true, there’s a 10% chance of being in one of the two tails, so our testing error is actually α = 0.10, not 0.05. That is, not being careful about when to use one-sided tests effectively undermines the methods we’re working so hard to develop and utilize.", "response": "202 CHAPTER 5. FOUNDATIONS FOR INFERENCE Exercises"}
{"instruction": "EXAMPLE 7.40 College departments commonly run multiple lectures of the same introductory course each semester because of high demand. Consider a statistics department that runs three lectures of an introductory statistics course. We might like to determine whether there are statistically significant differences in first exam scores in these three classes (A, B, and C). Describe appropriate hypotheses to determine whether there are any differences between the three classes. The hypotheses may be written in the following form: H0: The average score is identical in all lectures. Any observed difference is due to chance. Nota- tionally, we write µA = µB = µC. HA: The average score varies by class. We would reject the null hypothesis in favor of the alternative hypothesis if there were larger differences among the class averages than what we might expect from chance alone. Strong evidence favoring the alternative hypothesis in ANOVA is described by unusually large differences among the group means. We will soon learn that assessing the variability of the group means relative to the variability among individual observations within each group is key to ANOVA’s success.", "response": "286 CHAPTER 7. INFERENCE FOR NUMERICAL DATA"}
{"instruction": "5.2 Confidence intervals for a proportion . . . . . . . . . . . . . . . . . . . . . . . . . . . 181", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.3 Hypothesis testing for a proportion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 6 Inference for categorical data 206", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.2 Confidence intervals for a proportion", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.3 Hypothesis testing for a proportion", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.1.1 Point estimates and error Suppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as ˆp (pronounced p-hat1). Unless we collect responses from every individual in the population, p remains unknown, and we use ˆp as our estimate of p. The difference we observe from the poll versus the parameter is called the error in the estimate. Generally, the error consists of two aspects: sampling error and bias. Sampling error, sometimes called sampling uncertainty, describes how much an estimate will tend to vary from one sample to the next. For instance, the estimate from one sample might be 1% too low while in another it may be 3% too high. Much of statistics, including much of this book, is focused on understanding and quantifying sampling error, and we will find it useful to consider a sample’s size to help us quantify this error; the sample size is often represented by the letter n. Bias describes a systematic tendency to over- or under-estimate the true population value. For example, if we were taking a student poll asking about support for a new college stadium, we’d probably get a biased estimate of the stadium’s level of student support by wording the question as, Do you support your school by supporting funding for the new stadium? We try to minimize bias through thoughtful data collection procedures, which were discussed in Chapter 1 and are the topic of many other books.", "response": "shaded. (d) 7 degrees of freedom, area above 11.7 shaded. Upper tail"}
{"instruction": "5.2. CONFIDENCE INTERVALS FOR A PROPORTION 181", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.2 Confidence intervals for a proportion The sample proportion ˆp provides a single plausible value for the population proportion p. However, the sample proportion isn’t perfect and will have some standard error associated with it. When stating an estimate for the population proportion, it is better practice to provide a plausible range of values instead of supplying just the point estimate.", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.2.1 Capturing the population parameter Using only a point estimate is like fishing in a murky lake with a spear. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish. A confidence interval is like fishing with a net, and it represents a range of plausible values where we are likely to find the population parameter. If we report a point estimate ˆp, we probably will not hit the exact population proportion. On the other hand, if we report a range of plausible values, representing a confidence interval, we have a good shot at capturing the parameter. GUIDED PRACTICE 5.6 If we want to be very certain we capture the population proportion in an interval, should we use a wider interval or a smaller interval?6", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.2.2 Constructing a 95% confidence interval Our sample proportion ˆp is the most plausible value of the population proportion, so it makes sense to build a confidence interval around this point estimate. The standard error provides a guide for how large we should make the confidence interval. The standard error represents the standard deviation of the point estimate, and when the Cen- tral Limit Theorem conditions are satisfied, the point estimate closely follows a normal distribution. In a normal distribution, 95% of the data is within 1.96 standard deviations of the mean. Using this principle, we can construct a confidence interval that extends 1.96 standard errors from the sample proportion to be 95% confident that the interval captures the population proportion: point estimate ± 1.96 × SE ˆp ± 1.96 × r p(1 −p) n But what does “95% confident” mean? Suppose we took many samples and built a 95% confidence interval from each. Then about 95% of those intervals would contain the parameter, p. Figure 5.6 shows the process of creating 25 intervals from 25 samples from the simulation in Section 5.1.2, where 24 of the resulting confidence intervals contain the simulation’s population proportion of p = 0.88, and one interval does not. 6If we want to be more certain we will capture the fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.2. CONFIDENCE INTERVALS FOR A PROPORTION 183", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.2.3 Changing the confidence level Suppose we want to consider confidence intervals where the confidence level is higher than 95%, such as a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could use a slightly narrower interval than our original 95% interval. The 95% confidence interval structure provides guidance in how to make intervals with different confidence levels. The general 95% confidence interval for a point estimate that follows a normal distribution is point estimate ± 1.96 × SE There are three components to this interval: the point estimate, “1.96”, and the standard error. The choice of 1.96 × SE was based on capturing 95% of the data since the estimate is within 1.96 standard errors of the parameter about 95% of the time. The choice of 1.96 corresponds to a 95% confidence level. GUIDED PRACTICE 5.9 If X is a normally distributed random variable, what is the probability of the value X being within", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.2. CONFIDENCE INTERVALS FOR A PROPORTION 185", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.2.5 Interpreting confidence intervals In each of the examples, we described the confidence intervals by putting them into the context of the data and also using somewhat formal language: Solar. We are 90% confident that 87.1% to 90.4% of American adults support the expansion of solar power in 2018. Ebola. We are 95% confident that the proportion of New York adults in October 2014 who sup- ported a quarantine for anyone who had come into contact with an Ebola patient was between", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.2. CONFIDENCE INTERVALS FOR A PROPORTION 187 Exercises", "response": "ebola survey →In New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in Guinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon thereafter, an NBC 4 New York/The Wall Street Journal/Marist Poll found that 82% of New Yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient”. This poll included responses of 1,042 New York adults between Oct 26th and 28th, 2014. Poll ID NY141026 on maristpoll.marist.edu."}
{"instruction": "5.7 Chronic illness, Part I. In 2013, the Pew Research Foundation reported that “45% of U.S. adults report that they live with one or more chronic conditions”.11 However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used in this setting. Create a 95% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study.", "response": "Recall that the general formula is point estimate ± z⋆× SE. First, identify the three different values. The point estimate is 45%, z⋆= 1.96 for a 95% confidence level, and SE = 1.2%. Then, plug the values into the formula: 45% ± 1.96 × 1.2% → (42.6%, 47.4%) We are 95% confident that the proportion of US adults who live with one or more chronic conditions is between"}
{"instruction": "5.9 Chronic illness, Part II. In 2013, the Pew Research Foundation reported that “45% of U.S. adults report that they live with one or more chronic conditions”, and the standard error for this estimate is 1.2%. Identify each of the following statements as true or false. Provide an explanation to justify each of your answers. (a) We can say with certainty that the confidence interval from Exercise 5.7 contains the true percentage of U.S. adults who suffer from a chronic illness. (b) If we repeated this study 1,000 times and constructed a 95% confidence interval for each study, then approximately 950 of those confidence intervals would contain the true fraction of U.S. adults who suffer from chronic illnesses. (c) The poll provides statistically significant evidence (at the α = 0.05 level) that the percentage of U.S. adults who suffer from chronic illnesses is below 50%. (d) Since the standard error is 1.2%, only 1.2% of people in the study communicated uncertainty about their answer.", "response": "(a) False. Confidence intervals provide a range of plausible values, and sometimes the truth is missed. A 95% confidence interval “misses” about 5% of the time. (b) True. Notice that the descrip- tion focuses on the true population value. (c) True. If we examine the 95% confidence interval computed in Exercise 5.7, we can see that 50% is not included in this interval. This means that in a hypothesis test, we would reject the null hypothesis that the propor- tion is 0.5. (d) False. The standard error describes the uncertainty in the overall estimate from natural fluctuations due to randomness, not the uncertainty corresponding to individuals’ responses."}
{"instruction": "5.11 Waiting at an ER, Part I. A hospital administrator hoping to improve wait times decides to estimate the average emergency room waiting time at her hospital. She collects a simple random sample of 64 patients and determines the time (in minutes) between when they checked in to the ER until they were first seen by a doctor. A 95% confidence interval based on this sample is (128 minutes, 147 minutes), which is based on the normal model for the mean. Determine whether the following statements are true or false, and explain your reasoning. (a) We are 95% confident that the average waiting time of these 64 emergency room patients is between 128 and 147 minutes. (b) We are 95% confident that the average waiting time of all patients at this hospital’s emergency room is between 128 and 147 minutes. (c) 95% of random samples have a sample mean between 128 and 147 minutes. (d) A 99% confidence interval would be narrower than the 95% confidence interval since we need to be more sure of our estimate. (e) The margin of error is 9.5 and the sample mean is 137.5. (f) In order to decrease the margin of error of a 95% confidence interval to half of what it is now, we would need to double the sample size. (Hint: the margin of error for a mean scales in the same way with sample size as the margin of error for a proportion.)", "response": "(a) False. The point estimate is always in the confidence interval, and this is a non-sensical use of a confidence interval with a point estimate (because the point estimate is, by design, listed within the con- fidence interval). (b) True. (c) False. The confidence interval is not about a sample mean. (d) False. To be more confident that we capture the parameter, we need a wider interval. Think about needing a bigger net to be more sure of catching a fish in a murky lake. (e) True. Optional explanation: This is true since the normal model was used to model the sample mean. The margin of error is half the width of the interval, and the sample mean is the midpoint of the interval. (f) False. In the calculation of the standard error, we divide the standard deviation by the square root of the sample size. To cut the SE (or margin of error) in half, we would need to sample 22 = 4 times the number of people in the initial sample."}
{"instruction": "5.12 Mental health. The General Social Survey asked the question: “For how many days during the past 30 days was your mental health, which includes stress, depression, and problems with emotions, not good?” Based on responses from 1,151 US residents, the survey reported a 95% confidence interval of 3.40 to 4.24 days in 2010. (a) Interpret this interval in context of the data. (b) What does “95% confident” mean? Explain in the context of the application. (c) Suppose the researchers think a 99% confidence level would be more appropriate for this interval. Will this new interval be smaller or wider than the 95% confidence interval? (d) If a new survey were to be done with 500 Americans, do you think the standard error of the estimate be larger, smaller, or about the same.", "response": ""}
{"instruction": "5.13 Website registration. A website is trying to increase registration for first-time visitors, exposing 1% of these visitors to a new site design. Of 752 randomly sampled visitors over a month who saw the new design, 64 registered. (a) Check any conditions required for constructing a confidence interval. (b) Compute the standard error. (c) Construct and interpret a 90% confidence interval for the fraction of first-time visitors of the site who would register under the new design (assuming stable behaviors by new visitors over time).", "response": "(a) The visitors are from a simple random sample, so independence is satisfied. The success- failure condition is also satisfied, with both 64 and 752 −64 = 688 above 10. Therefore, we can use a normal distribution to model ˆp and construct a confidence interval. (b) The sample proportion is ˆp = 64 752 = 0.085. The standard error is SE = r p(1 −p) n ≈ r ˆp(1 −ˆp) n = r"}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 189", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3 Hypothesis testing for a proportion The following question comes from a book written by Hans Rosling, Anna Rosling R¨onnlund, and Ola Rosling called Factfulness: How many of the world’s 1 year old children today have been vaccinated against some disease: a. 20% b. 50% c. 80% Write down what your answer (or guess), and when you’re ready, find the answer in the footnote.13 In this section, we’ll be exploring how people with a 4-year college degree perform on this and other world health questions as we learn about hypothesis tests, which are a framework used to rigorously evaluate competing ideas and claims.", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3.1 Hypothesis testing framework We’re interested in understanding how much people know about world health and development. If we take a multiple choice world health question, then we might like to understand if H0: People never learn these particular topics and their responses are simply equivalent to random guesses. HA: People have knowledge that helps them do better than random guessing, or perhaps, they have false knowledge that leads them to actually do worse than random guessing. These competing ideas are called hypotheses. We call H0 the null hypothesis and HA the alternative hypothesis. When there is a subscript 0 like in H0, data scientists pronounce it as “nought” (e.g. H0 is pronounced “H-nought”). NULL AND ALTERNATIVE HYPOTHESES The null hypothesis (H0) often represents a skeptical perspective or a claim to be tested. The alternative hypothesis (HA) represents an alternative claim under consideration and is often represented by a range of possible parameter values. Our job as data scientists is to play the role of a skeptic: before we buy into the alternative hypothesis, we need to see strong supporting evidence. The null hypothesis often represents a skeptical position or a perspective of “no difference”. In our first example, we’ll consider whether the typical person does any different than random guessing on Roslings’ question about infant vaccinations. The alternative hypothesis generally represents a new or stronger perspective. In the case of the question about infant vaccinations, it would certainly be interesting to learn whether people do better than random guessing, since that would mean that the typical person knows something about world health statistics. It would also be very interesting if we learned that people do worse than random guessing, which would suggest people believe incorrect information about world health. The hypothesis testing framework is a very general tool, and we often use it without a second thought. If a person makes a somewhat unbelievable claim, we are initially skeptical. However, if there is sufficient evidence that supports the claim, we set aside our skepticism and reject the null hypothesis in favor of the alternative. The hallmarks of hypothesis testing are also found in the US court system. 13The correct answer is (c): 80% of the world’s 1 year olds have been vaccinated against some disease.", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 191", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3.2 Testing hypotheses using confidence intervals We will use the rosling responses data set to evaluate the hypothesis test evaluating whether college-educated adults who get the question about infant vaccination correct is different from 33.3%. This data set summarizes the answers of 50 college-educated adults. Of these 50 adults, 24% of respondents got the question correct that 80% of 1 year olds have been vaccinated against some disease. Up until now, our discussion has been philosophical. However, now that we have data, we might ask ourselves: does the data provide strong evidence that the proportion of all college-educated adults who would answer this question correctly is different than 33.3%? We learned in Section 5.1 that there is fluctuation from one sample to another, and it is unlikely that our sample proportion, ˆp, will exactly equal p, but we want to make a conclusion about p. We have a nagging concern: is this deviation of 24% from 33.3% simply due to chance, or does the data provide strong evidence that the population proportion is different from 33.3%? In Section 5.2, we learned how to quantify the uncertainty in our estimate using confidence intervals. The same method for measuring variability can be useful for the hypothesis test. EXAMPLE 5.20 Check whether it is reasonable to construct a confidence interval for p using the sample data, and if so, construct a 95% confidence interval. The conditions are met for ˆp to be approximately normal: the data come from a simple random sample (satisfies independence), and nˆp = 12 and n(1 −ˆp) = 38 are both at least 10 (success-failure condition). To construct the confidence interval, we will need to identify the point estimate (ˆp = 0.24), the critical value for the 95% confidence level (z⋆= 1.96), and the standard error of ˆp (SEˆp = p ˆp(1 −ˆp)/n =", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 193", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3.3 Decision errors Hypothesis tests are not flawless: we can make an incorrect decision in a statistical hypothesis test based on the data. For example, in the court system innocent people are sometimes wrongly convicted and the guilty sometimes walk free. One key distinction with statistical hypothesis tests is that we have the tools necessary to probabilistically quantify how often we make errors in our conclusions. Recall that there are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios, which are summarized in Figure 5.8. Test conclusion do not reject H0 reject H0 in favor of HA H0 true okay Type 1 Error Truth HA true Type 2 Error okay Figure 5.8: Four different scenarios for hypothesis tests. A Type 1 Error is rejecting the null hypothesis when H0 is actually true. A Type 2 Error is failing to reject the null hypothesis when the alternative is actually true. GUIDED PRACTICE 5.25 In a US court, the defendant is either innocent (H0) or guilty (HA). What does a Type 1 Error represent in this context? What does a Type 2 Error represent? Figure 5.8 may be useful.19 EXAMPLE 5.26 How could we reduce the Type 1 Error rate in US courts? What influence would this have on the Type 2 Error rate? To lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors. GUIDED PRACTICE 5.27 How could we reduce the Type 2 Error rate in US courts? What influence would this have on the Type 1 Error rate?20 Exercises 5.25-5.27 provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type. Hypothesis testing is built around rejecting or failing to reject the null hypothesis. That is, we do not reject H0 unless we have strong evidence. But what precisely does strong evidence mean? As a general rule of thumb, for those cases where the null hypothesis is actually true, we do not want to incorrectly reject H0 more than 5% of the time. This corresponds to a significance level of", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3.4 Formal testing using p-values The p-value is a way of quantifying the strength of the evidence against the null hypothesis and in favor of the alternative hypothesis. Statistical hypothesis testing typically uses the p-value method rather than making a decision based on confidence intervals. P-VALUE The p-value is the probability of observing data at least as favorable to the alternative hy- pothesis as our current data set, if the null hypothesis were true. We typically use a summary statistic of the data, in this section the sample proportion, to help compute the p-value and evaluate the hypotheses. EXAMPLE 5.28 Pew Research asked a random sample of 1000 American adults whether they supported the increased usage of coal to produce energy. Set up hypotheses to evaluate whether a majority of American adults support or oppose the increased usage of coal. The uninteresting result is that there is no majority either way: half of Americans support and the other half oppose expanding the use of coal to produce energy. The alternative hypothesis would be that there is a majority support or oppose (though we do not known which one!) expanding the use of coal. If p represents the proportion supporting, then we can write the hypotheses as H0: p = 0.5 HA: p ̸= 0.5 In this case, the null value is p0 = 0.5. When evaluating hypotheses for proportions using the p-value method, we will slightly modify how we check the success-failure condition and compute the standard error for the single proportion case. These changes aren’t dramatic, but pay close attention to how we use the null value, p0.", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 195 EXAMPLE 5.29 Pew Research’s sample show that 37% of American adults support increased usage of coal. We now wonder, does 37% represent a real difference from the null hypothesis of 50%? What would the sampling distribution of ˆp look like if the null hypothesis were true? If the null hypothesis were true, the population proportion would be the null value, 0.5. We previ- ously learned that the sampling distribution of ˆp will be normal when two conditions are met: Independence. The poll was based on a simple random sample, so independence is satisfied. Success-failure. Based on the poll’s sample size of n = 1000, the success-failure condition is met, since np H0 = 1000 × 0.5 = 500 n(1 −p) H0 = 1000 × (1 −0.5) = 500 are both at least 10. Note that the success-failure condition was checked using the null value, p0 = 0.5; this is the first procedural difference from confidence intervals. If the null hypothesis were true, the sampling distribution indicates that a sample proportion based on n = 1000 observations would be normally distributed. Next, we can compute the standard error, where we will again use the null value p0 = 0.5 in the calculation: SEˆp = r p(1 −p) n H0 = r", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 197 COMPARE THE P-VALUE TO α TO EVALUATE H0 H0 H0 When the p-value is less than the significance level, α, reject H0. We would report a conclusion that the data provide strong evidence supporting the alternative hypothesis. When the p-value is greater than α, do not reject H0, and report that we do not have sufficient evidence to reject the null hypothesis. In either case, it is important to describe the conclusion in the context of the data. GUIDED PRACTICE 5.32 Do a majority of Americans support or oppose nuclear arms reduction? Set up hypotheses to evaluate this question.21 EXAMPLE 5.33 A simple random sample of 1028 US adults in March 2013 show that 56% support nuclear arms reduction. Does this provide convincing evidence that a majority of Americans supported nuclear arms reduction at the 5% significance level? First, check conditions: Independence. The poll was of a simple random sample of US adults, meaning the observations are independent. Success-failure. In a one-proportion hypothesis test, this condition is checked using the null pro- portion, which is p0 = 0.5 in this context: np0 = n(1 −p0) = 1028 × 0.5 = 514 ≥10. With these conditions verified, we can model ˆp using a normal model. Next the standard error can be computed. The null value p0 is used again here, because this is a hypothesis test for a single proportion. SEˆp = r p0(1 −p0) n = r", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3.5 Choosing a significance level Choosing a significance level for a test is important in many contexts, and the traditional level is α = 0.05. However, it can be helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test. If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01). Under this scenario we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring HA before we would reject H0. If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we might choose a higher significance level (e.g. 0.10). Here we want to be cautious about failing to reject H0 when the alternative hypothesis is actually true. Additionally, if the cost of collecting data is small relative to the cost of a Type 2 Error, then it may also be a good strategy to collect more data. Under this strategy, the Type 2 Error can be reduced while not affecting the Type 1 Error rate. Of course, collecting extra data is often costly, so there is typically a cost-benefit analysis to be considered. EXAMPLE 5.34 A car manufacturer is considering switching to a new, higher quality piece of equipment that con- structs vehicle door hinges. They figure that they will save money in the long run if this new machine produces hinges that have flaws less than 0.2% of the time. However, if the hinges are flawed more than 0.2% of the time, they wouldn’t get a good enough return-on-investment from the new piece of equipment, and they would lose money. Is there good reason to modify the significance level in such a hypothesis test? The null hypothesis would be that the rate of flawed hinges is 0.2%, while the alternative is that it the rate is different than 0.2%. This decision is just one of many that have a marginal impact on the car and company. A significance level of 0.05 seems reasonable since neither a Type 1 or Type 2 Error should be dangerous or (relatively) much more expensive.", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 199 EXAMPLE 5.35 The same car manufacturer is considering a slightly more expensive supplier for parts related to safety, not door hinges. If the durability of these safety components is shown to be better than the current supplier, they will switch manufacturers. Is there good reason to modify the significance level in such an evaluation? The null hypothesis would be that the suppliers’ parts are equally reliable. Because safety is involved, the car company should be eager to switch to the slightly more expensive manufacturer (reject H0), even if the evidence of increased safety is only moderately strong. A slightly larger significance level, such as α = 0.10, might be appropriate. GUIDED PRACTICE 5.36 A part inside of a machine is very expensive to replace. However, the machine usually functions properly even if this part is broken, so the part is replaced only if we are extremely certain it is broken based on a series of measurements. Identify appropriate hypotheses for this test (in plain language) and suggest an appropriate significance level.22 WHY IS 0.05 THE DEFAULT? The α = 0.05 threshold is most common. But why? Maybe the standard level should be smaller, or perhaps larger. If you’re a little puzzled, you’re reading with an extra critical eye – good job! We’ve made a 5-minute task to help clarify why 0.05: www.openintro.org/why05", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3.6 Statistical significance versus practical significance When the sample size becomes larger, point estimates become more precise and any real differ- ences in the mean and null value become easier to detect and recognize. Even a very small difference would likely be detected if we took a large enough sample. Sometimes researchers will take such large samples that even the slightest difference is detected, even differences where there is no practical value. In such cases, we still say the difference is statistically significant, but it is not practi- cally significant. For example, an online experiment might identify that placing additional ads on a movie review website statistically significantly increases viewership of a TV show by 0.001%, but this increase might not have any practical value. One role of a data scientist in conducting a study often includes planning the size of the study. The data scientist might first consult experts or scientific literature to learn what would be the smallest meaningful difference from the null value. She also would obtain other information, such as a very rough estimate of the true proportion p, so that she could roughly estimate the standard error. From here, she can suggest a sample size that is sufficiently large that, if there is a real difference that is meaningful, we could detect it. While larger sample sizes may still be used, these calculations are especially helpful when considering costs or potential risks, such as possible health impacts to volunteers in a medical study. 22Here the null hypothesis is that the part is not broken, and the alternative is that it is broken. If we don’t have sufficient evidence to reject H0, we would not replace the part. It sounds like failing to fix the part if it is broken (H0 false, HA true) is not very problematic, and replacing the part is expensive. Thus, we should require very strong evidence against H0 before we replace the part. Choose a small significance level, such as α = 0.01.", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3.7 One-sided hypothesis tests (special topic) So far we’ve only considered what are called two-sided hypothesis tests, where we care about detecting whether p is either above or below some null value p0. There is a second type of hypothesis test called a one-sided hypothesis test. For a one-sided hypothesis test, the hypotheses take one of the following forms: 1. There’s only value in detecting if the population parameter is less than some value p0. In this case, the alternative hypothesis is written as p < p0 for some null value p0. 2. There’s only value in detecting if the population parameter is more than some value p0: In this case, the alternative hypothesis is written as p > p0. While we adjust the form of the alternative hypothesis, we continue to write the null hypothesis using an equals-sign in the one-sided hypothesis test case. In the entire hypothesis testing procedure, there is only one difference in evaluating a one- sided hypothesis test vs a two-sided hypothesis test: how to compute the p-value. In a one-sided hypothesis test, we compute the p-value as the tail area in the direction of the alternative hypothesis only, meaning it is represented by a single tail area. Herein lies the reason why one-sided tests are sometimes interesting: if we don’t have to double the tail area to get the p-value, then the p-value is smaller and the level of evidence required to identify an interesting finding in the direction of the alternative hypothesis goes down. However, one-sided tests aren’t all sunshine and rainbows: the heavy price paid is that any interesting findings in the opposite direction must be disregarded. EXAMPLE 5.37 In Section 1.1, we encountered an example where doctors were interested in determining whether stents would help people who had a high risk of stroke. The researchers believed the stents would help. Unfortunately, the data showed the opposite: patients who received stents actually did worse. Why was using a two-sided test so important in this context? Before the study, researchers had reason to believe that stents would help patients since existing research suggested stents helped in patients with heart attacks. It would surely have been tempting to use a one-sided test in this situation, and had they done this, they would have limited their ability to identify potential harm to patients. Example 5.37 highlights that using a one-sided hypothesis creates a risk of overlooking data supporting the opposite conclusion. We could have made a similar error when reviewing the Roslings’ question data this section; if we had a pre-conceived notion that college-educated people wouldn’t do worse than random guessing and so used a one-sided test, we would have missed the really interesting finding that many people have incorrect knowledge about global public health. When might a one-sided test be appropriate to use? Very rarely. Should you ever find yourself considering using a one-sided test, carefully answer the following question: What would I, or others, conclude if the data happens to go clearly in the opposite direc- tion than my alternative hypothesis? If you or others would find any value in making a conclusion about the data that goes in the opposite direction of a one-sided test, then a two-sided hypothesis test should actually be used. These considerations can be subtle, so exercise caution. We will only apply two-sided tests in the rest of this book.", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 201 EXAMPLE 5.38 Why can’t we simply run a one-sided test that goes in the direction of the data? We’ve been building a careful framework that controls for the Type 1 Error, which is the significance level α in a hypothesis test. We’ll use the α = 0.05 below to keep things simple. Imagine we could pick the one-sided test after we saw the data. What will go wrong? • If ˆp is smaller than the null value, then a one-sided test where p < p0 would mean that any observation in the lower 5% tail of the null distribution would lead to us rejecting H0. • If ˆp is larger than the null value, then a one-sided test where p > p0 would mean that any observation in the upper 5% tail of the null distribution would lead to us rejecting H0. Then if H0 were true, there’s a 10% chance of being in one of the two tails, so our testing error is actually α = 0.10, not 0.05. That is, not being careful about when to use one-sided tests effectively undermines the methods we’re working so hard to develop and utilize.", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.17 Online communication. A study suggests that 60% of college student spend 10 or more hours per week communicating with others online. You believe that this is incorrect and decide to collect your own sample for a hypothesis test. You randomly sample 160 students from your dorm and find that 70% spent 10 or more hours a week communicating with others online. A friend of yours, who offers to help you with the hypothesis test, comes up with the following set of hypotheses. Indicate any errors you see. H0 : ˆp < 0.6 HA : ˆp > 0.7", "response": "(1) The hypotheses should be about the pop- ulation proportion (p), not the sample proportion. (2) The null hypothesis should have an equal sign. (3) The alternative hypothesis should have a not- equals sign, and (4) it should reference the null value, p0 = 0.6, not the observed sample propor- tion. The correct way to set up these hypotheses is: H0 : p = 0.6 and HA : p ̸= 0.6."}
{"instruction": "5.19 Cyberbullying rates. Teens were surveyed about cyberbullying, and 54% to 64% reported experienc- ing cyberbullying (95% confidence interval).23 Answer the following questions based on this interval. (a) A newspaper claims that a majority of teens have experienced cyberbullying. Is this claim supported by the confidence interval? Explain your reasoning. (b) A researcher conjectured that 70% of teens have experienced cyberbullying. Is this claim supported by the confidence interval? Explain your reasoning. (c) Without actually calculating the interval, determine if the claim of the researcher from part (b) would be supported based on a 90% confidence interval? 23Pew Research Center, A Majority of Teens Have Experienced Some Form of Cyberbullying. September 27, 2018.", "response": ""}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 203", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.21 Minimum wage, Part I. Do a majority of US adults believe raising the minimum wage will help the economy, or is there a majority who do not believe this? A Rasmussen Reports survey of a random sample of 1,000 US adults found that 42% believe it will help the economy.24 Conduct an appropriate hypothesis test to help answer the research question.", "response": "(i) Set up hypotheses. H0: p = 0.5, HA: p ̸= 0.5. We will use a significance level of α = 0.05. (ii) Check conditions: simple random sample gets us independence, and the success-failure conditions is satisfied since 0.5 × 1000 = 500 for each group is at least 10. (iii) Next, we calculate: SE = p"}
{"instruction": "5.23 Working backwards, Part I. You are given the following hypotheses: H0 : p = 0.3 HA : p ̸= 0.3 We know the sample size is 90. For what sample proportion would the p-value be equal to 0.05? Assume that all conditions necessary for inference are satisfied.", "response": "If the p-value is 0.05, this means the test statis- tic would be either Z = −1.96 or Z = 1.96. We’ll show the calculations for Z = 1.96. Standard er- ror: SE = p"}
{"instruction": "5.27 Relaxing after work. The General Social Survey asked the question: “After an average work day, about how many hours do you have to relax or pursue activities that you enjoy?” to a random sample of 1,155 Americans.25 A 95% confidence interval for the mean number of hours spent relaxing or pursuing activities they enjoy was (1.38, 1.92). (a) Interpret this interval in context of the data. (b) Suppose another set of researchers reported a confidence interval with a larger margin of error based on the same sample of 1,155 Americans. How does their confidence level compare to the confidence level of the interval stated above? (c) Suppose next year a new survey asking the same question is conducted, and this time the sample size is 2,500. Assuming that the population characteristics, with respect to how much time people spend relaxing after work, have not changed much within a year. How will the margin of error of the 95% confidence interval constructed based on data from the new survey compare to the margin of error of the interval stated above?", "response": "(a) We are 95% confident that Americans spend an average of 1.38 to 1.92 hours per day relax- ing or pursuing activities they enjoy. (b) Their con- fidence level must be higher as the width of the con- fidence interval increases as the confidence level in- creases. (c) The new margin of error will be smaller, since as the sample size increases, the standard error decreases, which will decrease the margin of error."}
{"instruction": "5.29 Testing for food safety. A food safety inspector is called upon to investigate a restaurant with a few customer reports of poor sanitation practices. The food safety inspector uses a hypothesis testing framework to evaluate whether regulations are not being met. If he decides the restaurant is in gross violation, its license to serve food will be revoked. (a) Write the hypotheses in words. (b) What is a Type 1 Error in this context? (c) What is a Type 2 Error in this context? (d) Which error is more problematic for the restaurant owner? Why? (e) Which error is more problematic for the diners? Why? (f) As a diner, would you prefer that the food safety inspector requires strong evidence or very strong evidence of health concerns before revoking a restaurant’s license? Explain your reasoning.", "response": "(a) H0: The restaurant meets food safety and sanitation regulations. HA: The restaurant does not meet food safety and sanitation regulations. (b) The food safety inspector concludes that the restaurant does not meet food safety and sanitation regulations and shuts down the restaurant when the restaurant is actually safe. (c) The food safety inspector concludes that the restaurant meets food safety and sanitation regulations and the restaurant stays open when the restaurant is actually not safe. (d) A Type 1 Error may be more problematic for the restaurant owner since his restaurant gets shut down even though it meets the food safety and sanitation regulations. (e) A Type 2 Error may be more problematic for diners since the restaurant deemed safe by the in- spector is actually not. (f) Strong evidence. Diners would rather a restaurant that meet the regulations get shut down than a restaurant that doesn’t meet the regulations not get shut down."}
{"instruction": "5.31 Unemployment and relationship problems. A USA Today/Gallup poll asked a group of unemployed and underemployed Americans if they have had major problems in their relationships with their spouse or another close family member as a result of not having a job (if unemployed) or not having a full-time job (if underemployed). 27% of the 1,145 unemployed respondents and 25% of the 675 underemployed respondents said they had major problems in relationships as a result of their employment status. (a) What are the hypotheses for evaluating if the proportions of unemployed and underemployed people who had relationship problems were different? (b) The p-value for this hypothesis test is approximately 0.35. Explain what this means in context of the hypothesis test and the data. 25National Opinion Research Center, General Social Survey, 2018.", "response": "25"}
{"instruction": "5.3. HYPOTHESIS TESTING FOR A PROPORTION 205", "response": "stent30, stent365 →These data sets are described in Data Appendix B.1."}
{"instruction": "5.33 Nutrition labels. The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random sample of 35 bags yielded a confidence interval for the number of calories per bag of 128.2 to 139.8 calories. Is there evidence that the nutrition label does not provide an accurate measure of calories in the bags of potato chips?", "response": "Because 130 is inside the confidence interval, we do not have convincing evidence that the true av- erage is any different than what the nutrition label suggests."}
{"instruction": "5.36 Same observation, different sample size. Suppose you conduct a hypothesis test based on a sample where the sample size is n = 50, and arrive at a p-value of 0.08. You then refer back to your notes and discover that you made a careless mistake, the sample size should have been n = 500. Will your p-value increase, decrease, or stay the same? Explain.", "response": "and the p-value is approximately 0. With such a small p-value, we reject H0. The data provide con- vincing evidence that the average weight of chicks varies across some (or all) feed supplement groups."}
{"instruction": "5.37 Gender pay gap in medicine. A study examined the average pay for men and women entering the workforce as doctors for 21 different positions.26 (a) If each gender was equally paid, then we would expect about half of those positions to have men paid more than women and women would be paid more than men in the other half of positions. Write appropriate hypotheses to test this scenario. (b) Men were, on average, paid more in 19 of those 21 positions. Supposing these 21 positions represent a simple random sample, complete a hypothesis test using your hypotheses from part (a). 26Lo Sasso AT et al. “The $16,819 Pay Gap For Newly Trained Physicians: The Unexplained Trend Of Men Earning More Than Women”. In: Health Affairs 30.2 (2011).", "response": "(a) In effect, we’re checking whether men are paid more than women (or vice-versa), and we’d ex- pect these outcomes with either chance under the null hypothesis: H0 : p = 0.5 HA : p ̸= 0.5 We’ll use p to represent the fraction of cases where men are paid more than women. (b) Below is the completion of the hypothesis test. • There isn’t a good way to check independence here since the jobs are not a simple random sample. However, independence doesn’t seem unreasonable, since the individuals in each job are different from each other. The success- failure condition is met since we check it using the null proportion: p0n = (1 −p0)n = 10.5 is greater than 10. • We can compute the sample proportion, SE, and test statistic: ˆp = 19/21 = 0.905 SE = r"}
{"instruction": "7.1 One-sample means with the t-distribution . . . . . . . . . . . . . . . . . . . . . . . . 251", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1 One-sample means with the t-distribution", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1. ONE-SAMPLE MEANS WITH THE t-DISTRIBUTION 251", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1 One-sample means with the t-distribution Similar to how we can model the behavior of the sample proportion ˆp using a normal distribution, the sample mean ¯x can also be modeled using a normal distribution when certain conditions are met. However, we’ll soon learn that a new distribution, called the t-distribution, tends to be more useful when working with the sample mean. We’ll first learn about this new distribution, then we’ll use it to construct confidence intervals and conduct hypothesis tests for the mean.", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1.1 The sampling distribution of ¯x¯x¯x The sample mean tends to follow a normal distribution centered at the population mean, µ, when certain conditions are met. Additionally, we can compute a standard error for the sample mean using the population standard deviation σ and the sample size n. CENTRAL LIMIT THEOREM FOR THE SAMPLE MEAN When we collect a sufficiently large sample of n independent observations from a population with mean µ and standard deviation σ, the sampling distribution of ¯x will be nearly normal with Mean = µ Standard Error (SE) = σ √n Before diving into confidence intervals and hypothesis tests using ¯x, we first need to cover two topics: • When we modeled ˆp using the normal distribution, certain conditions had to be satisfied. The conditions for working with ¯x are a little more complex, and we’ll spend Section 7.1.2 discussing how to check conditions for inference. • The standard error is dependent on the population standard deviation, σ. However, we rarely know σ, and instead we must estimate it. Because this estimation is itself imperfect, we use a new distribution called the t-distribution to fix this problem, which we discuss in Section 7.1.3.", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1. ONE-SAMPLE MEANS WITH THE t-DISTRIBUTION 253 We’ll find it useful to use a new distribution for inference calculations called the t-distribution. A t-distribution, shown as a solid line in Figure 7.1, has a bell shape. However, its tails are thicker than the normal distribution’s, meaning observations are more likely to fall beyond two standard de- viations from the mean than under the normal distribution. The extra thick tails of the t-distribution are exactly the correction needed to resolve the problem of using s in place of σ in the SE calculation. −4 −2 0 2 4 Normal t−distribution Figure 7.1: Comparison of a t-distribution and a normal distribution. The t-distribution is always centered at zero and has a single parameter: degrees of freedom. The degrees of freedom (df df df) describes the precise form of the bell-shaped t-distribution. Several t-distributions are shown in Figure 7.2 in comparison to the normal distribution. In general, we’ll use a t-distribution with df = n−1 to model the sample mean when the sample size is n. That is, when we have more observations, the degrees of freedom will be larger and the t-distribution will look more like the standard normal distribution; when the degrees of freedom is about 30 or more, the t-distribution is nearly indistinguishable from the normal distribution. −4 −2 0 2 4 normal t, df = 8 t, df = 4 t, df = 2 t, df = 1 Figure 7.2: The larger the degrees of freedom, the more closely the t-distribution resembles the standard normal distribution. DEGREES OF FREEDOM (df df df) The degrees of freedom describes the shape of the t-distribution. The larger the degrees of freedom, the more closely the distribution approximates the normal model. When modeling ¯x using the t-distribution, use df = n −1. The t-distribution allows us greater flexibility than the normal distribution when analyzing numerical data. In practice, it’s common to use statistical software, such as R, Python, or SAS for these analyses. Alternatively, a graphing calculator or a t-table may be used; the t-table is similar to the normal distribution table, and it may be found in Appendix C.2, which includes usage instructions and examples for those who wish to use this option. No matter the approach you choose, apply your method using the examples below to confirm your working understanding of the t-distribution.", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1. ONE-SAMPLE MEANS WITH THE t-DISTRIBUTION 255", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1.4 One sample t-confidence intervals Let’s get our first taste of applying the t-distribution in the context of an example about the mercury content of dolphin muscle. Elevated mercury concentrations are an important problem for both dolphins and other animals, like humans, who occasionally eat them. Figure 7.5: A Risso’s dolphin. —————————– Photo by Mike Baird (www.bairdphotos.com). CC BY 2.0 license. We will identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso’s dolphins from the Taiji area in Japan. The data are summarized in Figure 7.6. The minimum and maximum observed values can be used to evaluate whether or not there are clear outliers. n ¯x s minimum maximum 19", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1. ONE-SAMPLE MEANS WITH THE t-DISTRIBUTION 257 CONFIDENCE INTERVAL FOR A SINGLE MEAN Once you’ve determined a one-mean confidence interval would be helpful for an application, there are four steps to constructing the interval: Prepare. Identify ¯x, s, n, and determine what confidence level you wish to use. Check. Verify the conditions to ensure ¯x is nearly normal. Calculate. If the conditions hold, compute SE, find t⋆ df, and construct the interval. Conclude. Interpret the confidence interval in the context of the problem. GUIDED PRACTICE 7.12 Using the information and results of Guided Practice 7.10 and Example 7.11, compute a 90% con- fidence interval for the average mercury content of croaker white fish (Pacific).4 GUIDED PRACTICE 7.13 The 90% confidence interval from Guided Practice 7.12 is 0.256 ppm to 0.318 ppm. Can we say that 90% of croaker white fish (Pacific) have mercury levels between 0.256 and 0.318 ppm?5", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1.5 One sample t-tests Is the typical US runner getting faster or slower over time? We consider this question in the context of the Cherry Blossom Race, which is a 10-mile race in Washington, DC each spring. The average time for all runners who finished the Cherry Blossom Race in 2006 was 93.29 minutes (93 minutes and about 17 seconds). We want to determine using data from 100 participants in the 2017 Cherry Blossom Race whether runners in this race are getting faster or slower, versus the other possibility that there has been no change. GUIDED PRACTICE 7.14 What are appropriate hypotheses for this context?6 GUIDED PRACTICE 7.15 The data come from a simple random sample of all participants, so the observations are independent. However, should we be worried about the normality condition? See Figure 7.7 for a histogram of the differences and evaluate if we can move forward.7 When completing a hypothesis test for the one-sample mean, the process is nearly identical to completing a hypothesis test for a single proportion. First, we find the Z-score using the observed value, null value, and standard error; however, we call it a T-score since we use a t-distribution for calculating the tail area. Then we find the p-value using the same ideas we used previously: find the one-tail area under the sampling distribution, and double it. 4¯x ± t⋆ 14 × SE →0.287 ± 1.76 × 0.0178 →(0.256, 0.318). We are 90% confident that the average mercury content of croaker white fish (Pacific) is between 0.256 and 0.318 ppm. 5No, a confidence interval only provides a range of plausible values for a population parameter, in this case the population mean. It does not describe what we might observe for individual observations. 6H0: The average 10-mile run time was the same for 2006 and 2017. µ = 93.29 minutes. HA: The average 10-mile run time for 2017 was different than that of 2006. µ ̸= 93.29 minutes. 7With a sample of 100, we should only be concerned if there is are particularly extreme outliers. The histogram of the data doesn’t show any outliers of concern (and arguably, no outliers at all).", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1. ONE-SAMPLE MEANS WITH THE t-DISTRIBUTION 259 Exercises", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.1 Identify the critical t. An independent random sample is selected from an approximately normal population with unknown standard deviation. Find the degrees of freedom and the critical t-value (t⋆) for the given sample size and confidence level. (a) n = 6, CL = 90% (b) n = 21, CL = 98% (c) n = 29, CL = 95% (d) n = 12, CL = 99%", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.3 Find the p-value, Part I. An independent random sample is selected from an approximately normal population with an unknown standard deviation. Find the p-value for the given sample size and test statistic. Also determine if the null hypothesis would be rejected at α = 0.05. (a) n = 11, T = 1.91 (b) n = 17, T = −3.45 (c) n = 7, T = 0.83 (d) n = 28, T = 2.13", "response": "Exam versions →This example was made up."}
{"instruction": "7.4 Find the p-value, Part II. An independent random sample is selected from an approximately normal population with an unknown standard deviation. Find the p-value for the given sample size and test statistic. Also determine if the null hypothesis would be rejected at α = 0.01. (a) n = 26, T = 2.485 (b) n = 18, T = 0.5", "response": "Blood pressure statistics →The blood pressure standard deviation for patients with blood pressure ranging from 140 to 180 mmHg is guessed and may be a little (but likely not dramat- ically) imprecise from what we’d observe in actual data."}
{"instruction": "7.5 Working backwards, Part I. A 95% confidence interval for a population mean, µ, is given as (18.985,", "response": "classdata →This example was made up."}
{"instruction": "7.9 Find the mean. You are given the following hypotheses: H0 : µ = 60 HA : µ ̸= 60 We know that the sample standard deviation is 8 and the sample size is 20. For what sample mean would the p-value be equal to 0.05? Assume that all conditions necessary for inference are satisfied. 8G. Heinz et al. “Exploring relationships in body dimensions”. In: Journal of Statistics Education 11.2 (2003).", "response": "T is either -2.09 or 2.09. Then ¯x is one of the following: −2.09 = ¯x −60 8 √ 20 →¯x = 56.26"}
{"instruction": "7.1. ONE-SAMPLE MEANS WITH THE t-DISTRIBUTION 261", "response": "run17 →www.cherryblossom.org"}
{"instruction": "7.11 Play the piano. Georgianna claims that in a small city renowned for its music school, the average child takes less than 5 years of piano lessons. We have a random sample of 20 children from the city, with a mean of 4.6 years of piano lessons and a standard deviation of 2.2 years. (a) Evaluate Georgianna’s claim (or that the opposite might be true) using a hypothesis test. (b) Construct a 95% confidence interval for the number of years students in this city take piano lessons, and interpret it in context of the data. (c) Do your results from the hypothesis test and the confidence interval agree? Explain your reasoning.", "response": "(a) We will conduct a 1-sample t-test. H0: µ = 5. HA: µ ̸= 5. We’ll use α = 0.05. This is a random sample, so the observations are inde- pendent. To proceed, we assume the distribution of years of piano lessons is approximately normal. SE = 2.2/ √ 20 = 0.4919. The test statistic is T = (4.6 −5)/SE = −0.81. df = 20 −1 = 19. The one-tail area is about 0.21, so the p-value is about"}
{"instruction": "7.13 Car insurance savings. A market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation of savings is $100. He wants to collect data such that he can get a margin of error of no more than $10 at a 95% confidence level. How large of a sample should he collect?", "response": "If the sample is large, then the margin of error will be about 1.96 × 100/√n. We want this value to be less than 10, which leads to n ≥384.16, meaning we need a sample size of at least 385 (round up for sample size calculations!)."}
{"instruction": "7.19 Global warming, Part I. Let’s consider a limited set of climate data, examining temperature differences in 1948 vs 2018. We sampled 197 locations from the National Oceanic and Atmospheric Administration’s (NOAA) historical data, where the data was available for both years of interest. We want to know: were there more days with temperatures exceeding 90°F in 2018 or in 1948?12 The difference in number of days exceeding 90°F (number of days in 2018 - number of days in 1948) was calculated for each of the 197 locations. The average of these differences was 2.9 days with a standard deviation of 17.2 days. We are interested in determining whether these data provide strong evidence that there were more days in 2018 that exceeded 90°F from NOAA’s weather stations. (a) Is there a relationship between the observations collected in 1948 and 2018? Or are the observations in the two groups independent? Explain. (b) Write hypotheses for this research in symbols and in words. (c) Check the conditions required to complete this test. A histogram of the differences is given to the right. (d) Calculate the test statistic and find the p-value. (e) Use α = 0.05 to evaluate the test, and interpret your conclusion in context. (f) What type of error might we have made? Explain in context what the error means. (g) Based on the results of this hypothesis test, would you expect a confidence interval for the average difference between the number of days exceeding 90°F from 1948 and 2018 to include 0? Explain your reasoning. Differences in Number of Days −60 −40 −20 0 20 40 60 0 10 20 30 40 50 −60 −40 −20 0 20 40 60 12NOAA, www.ncdc.noaa.gov/cdo-web/datasets, April 24, 2019.", "response": "(a) For each observation in one data set, there is exactly one specially corresponding observation in the other data set for the same geographic location. The data are paired. (b) H0 : µdiff = 0 (There is no difference in average number of days exceeding 90°F in 1948 and 2018 for NOAA stations.) HA : µdiff ̸= 0 (There is a difference.) (c) Locations were randomly sampled, so independence is reasonable. The sample size is at least 30, so we’re just looking for partic- ularly extreme outliers: none are present (the ob- servation off left in the histogram would be con- sidered a clear outlier, but not a particularly ex- treme one). Therefore, the conditions are satisfied. (d) SE = 17.2/ √ 197 = 1.23. T = 2.9−0"}
{"instruction": "7.20 High School and Beyond, Part I. The National Center of Education Statistics conducted a survey of high school seniors, collecting test data on reading, writing, and several other subjects. Here we examine a simple random sample of 200 students from this survey. Side-by-side box plots of reading and writing scores as well as a histogram of the differences in scores are shown below. scores read write 20 40 60 80 Differences in scores (read − write) −20 −10 0 10 20 0 10 20 30 40 (a) Is there a clear difference in the average reading and writing scores? (b) Are the reading and writing scores of each student independent of each other? (c) Create hypotheses appropriate for the following research question: is there an evident difference in the average scores of students in the reading and writing exam? (d) Check the conditions required to complete this test. (e) The average observed difference in scores is ¯xread−write = −0.545, and the standard deviation of the differences is 8.887 points. Do these data provide convincing evidence of a difference between the average scores on the two exams? (f) What type of error might we have made? Explain what the error means in the context of the application. (g) Based on the results of this hypothesis test, would you expect a confidence interval for the average difference between the reading and writing scores to include 0? Explain your reasoning.", "response": ""}
{"instruction": "7.21 Global warming, Part II. We considered the change in the number of days exceeding 90°F from 1948 and 2018 at 197 randomly sampled locations from the NOAA database in Exercise 7.19. The mean and standard deviation of the reported differences are 2.9 days and 17.2 days. (a) Calculate a 90% confidence interval for the average difference between number of days exceeding 90°F between 1948 and 2018. We’ve already checked the conditions for you. (b) Interpret the interval in context. (c) Does the confidence interval provide convincing evidence that there were more days exceeding 90°F in 2018 than in 1948 at NOAA stations? Explain.", "response": "(a) SE = 1.23 and t⋆= 1.65."}
{"instruction": "7.3.1 Confidence interval for a difference of means Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack? Figure 7.11 contains summary statistics for an experiment to test ESCs in sheep that had a heart attack. Each of these sheep was randomly assigned to the ESC or control group, and the change in their hearts’ pumping capacity was measured in the study. Figure 7.12 provides histograms of the two data sets. A positive value corresponds to increased pumping capacity, which generally suggests a stronger recovery. Our goal will be to identify a 95% confidence interval for the effect of ESCs on the change in heart pumping capacity relative to the control group. n ¯x s ESCs 9", "response": "Exam versions →This example was made up."}
{"instruction": "7.3. DIFFERENCE OF TWO MEANS 269 EXAMPLE 7.22 Calculate a 95% confidence interval for the effect of ESCs on the change in heart pumping capacity of sheep after they’ve suffered a heart attack. We will use the sample difference and the standard error that we computed earlier calculations: ¯xesc −¯xcontrol = 7.83 SE = r", "response": "Exam versions →This example was made up."}
{"instruction": "7.3.2 Hypothesis tests for the difference of two means A data set called ncbirths represents a random sample of 150 cases of mothers and their new- borns in North Carolina over a year. Four cases from this data set are represented in Figure 7.13. We are particularly interested in two variables: weight and smoke. The weight variable represents the weights of the newborns and the smoke variable describes which mothers smoked during pregnancy. We would like to know, is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke? We will use the North Carolina sample to try to answer this question. The smoking group includes 50 cases and the nonsmoking group contains 100 cases. fage mage weeks weight sex smoke 1 NA 13 37", "response": "Exam versions →This example was made up."}
{"instruction": "7.3. DIFFERENCE OF TWO MEANS 271 EXAMPLE 7.25 Complete the hypothesis test started in Example 7.23 and Guided Practice 7.24. Use a significance level of α = 0.05. For reference, ¯xn −¯xs = 0.40, SE = 0.26, and the sample sizes were nn = 100 and ns = 50. We can find the test statistic for this test using the values from Guided Practice 7.24: T = 0.40 −0", "response": "Exam versions →This example was made up."}
{"instruction": "7.3. DIFFERENCE OF TWO MEANS 273 −3 −2 −1 0 1 2 3 T = 1.15 Figure 7.17: The t-distribution with 26 degrees of freedom and the p-value from exam example represented as the shaded areas. EXAMPLE 7.30 Identify the p-value depicted in Figure 7.17 using df = 26, and provide a conclusion in the context of the case study. Using software, we can find the one-tail area (0.13) and then double this value to get the two-tail area, which is the p-value: 0.26. (Alternatively, we could use the t-table in Appendix C.2.) In Guided Practice 7.28, we specified that we would use α = 0.01. Since the p-value is larger than α, we do not reject the null hypothesis. That is, the data do not convincingly show that one exam version is more difficult than the other, and the teacher should not be convinced that she should add points to the Version B exam scores.", "response": "Exam versions →This example was made up."}
{"instruction": "7.23 Friday the 13th, Part I. In the early 1990’s, researchers in the UK collected data on traffic flow, number of shoppers, and traffic accident related emergency room admissions on Friday the 13th and the previous Friday, Friday the 6th. The histograms below show the distribution of number of cars passing by a specific intersection on Friday the 6th and Friday the 13th for many such date pairs. Also given are some sample statistics, where the difference is the number of cars on the 6th minus the number of cars on the 13th.19 Friday the 6th 120000 130000 140000 0 1 2 3 4 Friday the 13th 120000 130000 140000 0 1 2 3 Difference 0 2000 4000 0 1 2 3 4 5 6th 13th Diff. ¯x 128,385 126,550 1,835 s 7,259 7,664 1,176 n 10 10 10 (a) Are there any underlying structures in these data that should be considered in an analysis? Explain. (b) What are the hypotheses for evaluating whether the number of people out on Friday the 6th is different than the number out on Friday the 13th? (c) Check conditions to carry out the hypothesis test from part (b). (d) Calculate the test statistic and the p-value. (e) What is the conclusion of the hypothesis test? (f) Interpret the p-value in this context. (g) What type of error might have been made in the conclusion of your test? Explain.", "response": ""}
{"instruction": "7.29 Chicken diet and weight, Part II. Casein is a common weight gain supplement for humans. Does it have an effect on chickens? Using data provided in Exercise 7.27, test the hypothesis that the average weight of chickens that were fed casein is different than the average weight of chickens that were fed soybean. If your hypothesis test yields a statistically significant result, discuss whether or not the higher average weight of chickens can be attributed to the casein diet. Assume that conditions for inference are satisfied.", "response": ""}
{"instruction": "7.4.1 Going through the motions of a test We’re going to go through the motions of a hypothesis test. This will help us frame our calculations for determining an appropriate sample size for the study. EXAMPLE 7.31 Suppose a pharmaceutical company has developed a new drug for lowering blood pressure, and they are preparing a clinical trial (experiment) to test the drug’s effectiveness. They recruit people who are taking a particular standard blood pressure medication. People in the control group will continue to take their current medication through generic-looking pills to ensure blinding. Write down the hypotheses for a two-sided hypothesis test in this context. Generally, clinical trials use a two-sided alternative hypothesis, so below are suitable hypotheses for this context: H0: The new drug performs exactly as well as the standard medication. µtrmt −µctrl = 0. HA: The new drug’s performance differs from the standard medication. µtrmt −µctrl ̸= 0. EXAMPLE 7.32 The researchers would like to run the clinical trial on patients with systolic blood pressures between 140 and 180 mmHg. Suppose previously published studies suggest that the standard deviation of the patients’ blood pressures will be about 12 mmHg and the distribution of patient blood pressures will be approximately symmetric.26 If we had 100 patients per group, what would be the approximate standard error for ¯xtrmt −¯xctrl? The standard error is calculated as follows: SE¯xtrmt−¯xctrl = s s2 trmt ntrmt + s2 ctrl nctrl = r 122 100 + 122 100 = 1.70 This may be an imperfect estimate of SE¯xtrmt−¯xctrl, since the standard deviation estimate we used may not be perfectly correct for this group of patients. However, it is sufficient for our purposes. 25Even though we don’t cover it explicitly, similar sample size planning is also helpful for observational studies. 26In this particular study, we’d generally measure each patient’s blood pressure at the beginning and end of the study, and then the outcome measurement for the study would be the average change in blood pressure. That is, both µtrmt and µctrl would represent average differences. This is what you might think of as a 2-sample paired testing structure, and we’d analyze it exactly just like a hypothesis test for a difference in the average change for patients. In the calculations we perform here, we’ll suppose that 12 mmHg is the predicted standard deviation of a patient’s blood pressure difference over the course of the study.", "response": "Blood pressure statistics →The blood pressure standard deviation for patients with blood pressure ranging from 140 to 180 mmHg is guessed and may be a little (but likely not dramat- ically) imprecise from what we’d observe in actual data."}
{"instruction": "7.4. POWER CALCULATIONS FOR A DIFFERENCE OF MEANS 279 EXAMPLE 7.33 What does the null distribution of ¯xtrmt −¯xctrl look like? The degrees of freedom are greater than 30, so the distribution of ¯xtrmt −¯xctrl will be approximately normal. The standard deviation of this distribution (the standard error) would be about 1.70, and under the null hypothesis, its mean would be 0. −9 −6 −3 0 3 6 9 xtrmt −xctrl Null distribution EXAMPLE 7.34 For what values of ¯xtrmt −¯xctrl would we reject the null hypothesis? For α = 0.05, we would reject H0 if the difference is in the lower 2.5% or upper 2.5% tail: Lower 2.5%: For the normal model, this is 1.96 standard errors below 0, so any difference smaller than −1.96 × 1.70 = −3.332 mmHg. Upper 2.5%: For the normal model, this is 1.96 standard errors above 0, so any difference larger than 1.96 × 1.70 = 3.332 mmHg. The boundaries of these rejection regions are shown below: −9 −6 −3 0 3 6 9 xtrmt −xctrl Null distribution Reject H0 Do not reject H0 Reject H0 Next, we’ll perform some hypothetical calculations to determine the probability we reject the null hypothesis, if the alternative hypothesis were actually true.", "response": "Blood pressure statistics →The blood pressure standard deviation for patients with blood pressure ranging from 140 to 180 mmHg is guessed and may be a little (but likely not dramat- ically) imprecise from what we’d observe in actual data."}
{"instruction": "7.4.3 Determining a proper sample size In the last example, we found that if we have a sample size of 100 in each group, we can only detect an effect size of 3 mmHg with a probability of about 0.42. Suppose the researchers moved forward and only used 100 patients per group, and the data did not support the alternative hypothesis, i.e. the researchers did not reject H0. This is a very bad situation to be in for a few reasons: • In the back of the researchers’ minds, they’d all be wondering, maybe there is a real and meaningful difference, but we weren’t able to detect it with such a small sample. • The company probably invested hundreds of millions of dollars in developing the new drug, so now they are left with great uncertainty about its potential since the experiment didn’t have a great shot at detecting effects that could still be important. • Patients were subjected to the drug, and we can’t even say with much certainty that the drug doesn’t help (or harm) patients. • Another clinical trial may need to be run to get a more conclusive answer as to whether the drug does hold any practical value, and conducting a second clinical trial may take years and many millions of dollars. We want to avoid this situation, so we need to determine an appropriate sample size to ensure we can be pretty confident that we’ll detect any effects that are practically important. As mentioned earlier, a change of 3 mmHg was deemed to be the minimum difference that was practically important. As a first step, we could calculate power for several different sample sizes. For instance, let’s try 500 patients per group. GUIDED PRACTICE 7.36 Calculate the power to detect a change of -3 mmHg when using a sample size of 500 per group.27 (a) Determine the standard error (recall that the standard deviation for patients was expected to be about 12 mmHg). (b) Identify the null distribution and rejection regions. (c) Identify the alternative distribution when µtrmt −µctrl = −3. (d) Compute the probability we reject the null hypothesis. The researchers decided 3 mmHg was the minimum difference that was practically important, and with a sample size of 500, we can be very certain (97.7% or better) that we will detect any such difference. We now have moved to another extreme where we are exposing an unnecessary number of patients to the new drug in the clinical trial. Not only is this ethically questionable, but it would also cost a lot more money than is necessary to be quite sure we’d detect any important effects. The most common practice is to identify the sample size where the power is around 80%, and sometimes 90%. Other values may be reasonable for a specific context, but 80% and 90% are most commonly targeted as a good balance between high power and not exposing too many patients to a new treatment (or wasting too much money). We could compute the power of the test at several other possible sample sizes until we find one that’s close to 80%, but there’s a better way. We should solve the problem backwards. 27(a) The standard error is given as SE = q 122 500 + 122 500 = 0.76. (b) & (c) The null distribution, rejection boundaries, and alternative distribution are shown below: −9 −6 −3 0 3 6 9 xtrmt −xctrl Null distribution Distribution with µtrmt −µctrl = −3 The rejection regions are the areas on the outside of the two dotted lines and are at ±0.76 × 1.96 = ±1.49. (d) The area of the alternative distribution where µtrmt −µctrl = −3 has been shaded. We compute the Z-score and find the tail area: Z = −1.49−(−3)", "response": "Blood pressure statistics →The blood pressure standard deviation for patients with blood pressure ranging from 140 to 180 mmHg is guessed and may be a little (but likely not dramat- ically) imprecise from what we’d observe in actual data."}
{"instruction": "7.5.1 Core ideas of ANOVA In this section, we will learn a new method called analysis of variance (ANOVA) and a new test statistic called F. ANOVA uses a single hypothesis test to check whether the means across many groups are equal: H0: The mean outcome is the same across all groups. In statistical notation, µ1 = µ2 = · · · = µk where µi represents the mean of the outcome for observations in category i. HA: At least one mean is different. Generally we must check three conditions on the data before performing ANOVA: • the observations are independent within and across groups, • the data within each group are nearly normal, and • the variability across the groups is about equal. When these three conditions are met, we may perform an ANOVA to determine whether the data provide strong evidence against the null hypothesis that all the µi are equal. EXAMPLE 7.40 College departments commonly run multiple lectures of the same introductory course each semester because of high demand. Consider a statistics department that runs three lectures of an introductory statistics course. We might like to determine whether there are statistically significant differences in first exam scores in these three classes (A, B, and C). Describe appropriate hypotheses to determine whether there are any differences between the three classes. The hypotheses may be written in the following form: H0: The average score is identical in all lectures. Any observed difference is due to chance. Nota- tionally, we write µA = µB = µC. HA: The average score varies by class. We would reject the null hypothesis in favor of the alternative hypothesis if there were larger differences among the class averages than what we might expect from chance alone. Strong evidence favoring the alternative hypothesis in ANOVA is described by unusually large differences among the group means. We will soon learn that assessing the variability of the group means relative to the variability among individual observations within each group is key to ANOVA’s success.", "response": "classdata →This example was made up."}
{"instruction": "7.5. COMPARING MANY MEANS WITH ANOVA 287 variable description name Player name team The abbreviated name of the player’s team position The player’s primary field position (OF, IF, C) AB Number of opportunities at bat H Number of hits HR Number of home runs RBI Number of runs batted in AVG Batting average, which is equal to H/AB OBP On-base percentage, which is roughly equal to the fraction of times a player gets on base or hits a home run Figure 7.21: Variables and their descriptions for the bat18 data set. GUIDED PRACTICE 7.42 The null hypothesis under consideration is the following: µOF = µIF = µC. Write the null and corresponding alternative hypotheses in plain language.30 EXAMPLE 7.43 The player positions have been divided into three groups: outfield (OF), infield (IF), and catcher (C). What would be an appropriate point estimate of the on-base percentage by outfielders, µOF? A good estimate of the on-base percentage by outfielders would be the sample average of OBP for just those players whose position is outfield: ¯xOF = 0.320. Figure 7.22 provides summary statistics for each group. A side-by-side box plot for the on- base percentage is shown in Figure 7.23. Notice that the variability appears to be approximately constant across groups; nearly constant variance across groups is an important assumption that must be satisfied before we consider the ANOVA approach. OF IF C Sample size (ni) 160 205 64 Sample mean (¯xi)", "response": "classdata →This example was made up."}
{"instruction": "7.5.3 Analysis of variance (ANOVA) and the F -test The method of analysis of variance in this context focuses on answering one question: is the variability in the sample means so large that it seems unlikely to be from chance alone? This question is different from earlier testing procedures since we will simultaneously consider many groups, and evaluate whether their sample means differ more than we would expect from natural variation. We call this variability the mean square between groups (MSG), and it has an associated degrees of freedom, dfG = k −1 when there are k groups. The MSG can be thought of as a scaled variance formula for means. If the null hypothesis is true, any variation in the sample means is due to chance and shouldn’t be too large. Details of MSG calculations are provided in the footnote.32 However, we typically use software for these computations. The mean square between the groups is, on its own, quite useless in a hypothesis test. We need a benchmark value for how much variability should be expected among the sample means if the null hypothesis is true. To this end, we compute a pooled variance estimate, often abbreviated as the mean square error (MSE), which has an associated degrees of freedom value dfE = n −k. It is helpful to think of MSE as a measure of the variability within the groups. Details of the computations of the MSE and a link to an extra online section for ANOVA calculations are provided in the footnote33 for interested readers. When the null hypothesis is true, any differences among the sample means are only due to chance, and the MSG and MSE should be about equal. As a test statistic for ANOVA, we examine the fraction of MSG and MSE: F = MSG MSE The MSG represents a measure of the between-group variability, and MSE measures the variability within each of the groups. GUIDED PRACTICE 7.45 For the baseball data, MSG = 0.00803 and MSE = 0.00158. Identify the degrees of freedom associated with MSG and MSE and verify the F statistic is approximately 5.077.34 We can use the F statistic to evaluate the hypotheses in what is called an F -test. A p-value can be computed from the F statistic using an F distribution, which has two associated parameters: df1 and df2. For the F statistic in ANOVA, df1 = dfG and df2 = dfE. An F distribution with 2 and 426 degrees of freedom, corresponding to the F statistic for the baseball hypothesis test, is shown in Figure 7.24. 32Let ¯x represent the mean of outcomes across all groups. Then the mean square between groups is computed as MSG = 1 dfG SSG = 1 k −1 k X i=1 ni (¯xi −¯x)2 where SSG is called the sum of squares between groups and ni is the sample size of group i. 33Let ¯x represent the mean of outcomes across all groups. Then the sum of squares total (SST) is computed as SST = n X i=1 (xi −¯x)2 where the sum is over all observations in the data set. Then we compute the sum of squared errors (SSE) in one of two equivalent ways: SSE = SST −SSG = (n1 −1)s2 1 + (n2 −1)s2 2 + · · · + (nk −1)s2 k where s2 i is the sample variance (square of the standard deviation) of the residuals in group i. Then the MSE is the standardized form of SSE: MSE = 1 dfE SSE. For additional details on ANOVA calculations, see www.openintro.org/d?file=stat extra anova calculations 34There are k = 3 groups, so dfG = k −1 = 2. There are n = n1 + n2 + n3 = 429 total observations, so dfE = n−k = 426. Then the F statistic is computed as the ratio of MSG and MSE: F = MSG MSE = 0.00803", "response": "classdata →This example was made up."}
{"instruction": "7.5.4 Reading an ANOVA table from software The calculations required to perform an ANOVA by hand are tedious and prone to human error. For these reasons, it is common to use statistical software to calculate the F statistic and p-value. An ANOVA can be summarized in a table very similar to that of a regression summary, which we will see in Chapters 8 and 9. Figure 7.25 shows an ANOVA summary to test whether the mean of on-base percentage varies by player positions in the MLB. Many of these values should look familiar; in particular, the F-test statistic and p-value can be retrieved from the last two columns. Df Sum Sq Mean Sq F value Pr(>F) position 2", "response": "classdata →This example was made up."}
{"instruction": "7.5.5 Graphical diagnostics for an ANOVA analysis There are three conditions we must check for an ANOVA analysis: all observations must be independent, the data in each group must be nearly normal, and the variance within each group must be approximately equal. Independence. If the data are a simple random sample, this condition is satisfied. For processes and experiments, carefully consider whether the data may be independent (e.g. no pairing). For example, in the MLB data, the data were not sampled. However, there are not obvious reasons why independence would not hold for most or all observations. Approximately normal. As with one- and two-sample testing for means, the normality assump- tion is especially important when the sample size is quite small when it is ironically difficult to check for non-normality. A histogram of the observations from each group is shown in Fig- ure 7.26. Since each of the groups we’re considering have relatively large sample sizes, what we’re looking for are major outliers. None are apparent, so this conditions is reasonably met. Outfielders On−Base Percentage Frequency", "response": "classdata →This example was made up."}
{"instruction": "7.5.6 Multiple comparisons and controlling Type 1 Error rate When we reject the null hypothesis in an ANOVA analysis, we might wonder, which of these groups have different means? To answer this question, we compare the means of each possible pair of groups. For instance, if there are three groups and there is strong evidence that there are some differences in the group means, there are three comparisons to make: group 1 to group 2, group 1 to group 3, and group 2 to group 3. These comparisons can be accomplished using a two-sample t-test, but we use a modified significance level and a pooled estimate of the standard deviation across groups. Usually this pooled standard deviation can be found in the ANOVA table, e.g. along the bottom of Figure 7.25. EXAMPLE 7.47 Example 7.40 on page 285 discussed three statistics lectures, all taught during the same semester. Figure 7.27 shows summary statistics for these three courses, and a side-by-side box plot of the data is shown in Figure 7.28. We would like to conduct an ANOVA for these data. Do you see any deviations from the three conditions for ANOVA? In this case (like many others) it is difficult to check independence in a rigorous way. Instead, the best we can do is use common sense to consider reasons the assumption of independence may not hold. For instance, the independence assumption may not be reasonable if there is a star teaching assistant that only half of the students may access; such a scenario would divide a class into two subgroups. No such situations were evident for these particular data, and we believe that independence is acceptable. The distributions in the side-by-side box plot appear to be roughly symmetric and show no noticeable outliers. The box plots show approximately equal variability, which can be verified in Figure 7.27, supporting the constant variance assumption. Class i A B C ni 58 55 51 ¯xi", "response": "classdata →This example was made up."}
{"instruction": "7.43 True / False: ANOVA, Part I. Determine if the following statements are true or false in ANOVA, and explain your reasoning for statements you identify as false. (a) As the number of groups increases, the modified significance level for pairwise tests increases as well. (b) As the total sample size increases, the degrees of freedom for the residuals increases as well. (c) The constant variance condition can be somewhat relaxed when the sample sizes are relatively consistent across groups. (d) The independence assumption can be relaxed when the total sample size is large. 37National Opinion Research Center, General Social Survey, 2018.", "response": "(a) False. As the number of groups increases, so does the number of comparisons and hence the modified significance level decreases. (b) True. (c) True. (d) False. We need observations to be independent regardless of sample size."}
{"instruction": "7.2 n 50 50 50 Construct a 99% confidence interval for the average growth of (what had been) younger trees in the park over 2009-2019.", "response": "textbooks, ucla textbooks f18 →Data were collected by OpenIntro staff in 2010 and again in 2018. For the 2018 sample, we sampled 201 UCLA courses. Of those, 68 required books that could be found on Amazon. The websites where information was retrieved: sa.ucla.edu/ro/public/soc, ucla.verbacompare.com, and amazon.com."}
{"instruction": "7.53 Experiment resizing. At a startup company running a new weather app, an engineering team generally runs experiments where a random sample of 1% of the app’s visitors in the control group and another 1% were in the treatment group to test each new feature. The team’s core goal is to increase a metric called daily visitors, which is essentially the number of visitors to the app each day. They track this metric in each experiment arm and as their core experiment metric. In their most recent experiment, the team tested including a new animation when the app started, and the number of daily visitors in this experiment stabilized at +1.2% with a 95% confidence interval of (-0.2%, +2.6%). This means if this new app start animation was launched, the team thinks they might lose as many as 0.2% of daily visitors or gain as many as 2.6% more daily visitors. Suppose you are consulting as the team’s data scientist, and after discussing with the team, you and they agree that they should run another experiment that is bigger. You also agree that this new experiment should be able to detect a gain in the daily visitors metric of 1.0% or more with 80% power. Now they turn to you and ask, “How big of an experiment do we need to run to ensure we can detect this effect?” (a) How small must the standard error be if the team is to be able to detect an effect of 1.0% with 80% power and a significance level of α = 0.05? You may safely assume the percent change in daily visitors metric follows a normal distribution. (b) Consider the first experiment, where the point estimate was +1.2% and the 95% confidence interval was (-0.2%, +2.6%). If that point estimate followed a normal distribution, what was the standard error of the estimate? (c) The ratio of the standard error from part (a) vs the standard error from part (b) should be 1.97. How much bigger of an experiment is needed to shrink a standard error by a factor of 1.97? (d) Using your answer from part (c) and that the original experiment was a 1% vs 1% experiment to recommend an experiment size to the team.", "response": "(a) We should set 1.0% equal to 2.8 standard errors: 2.8 × SEdesired = 1.0% (see Example 7.37 on page 282 for details). This means the standard error should be about SE = 0.36% to achieve the desired statistical power. (b) The margin of error was 0.5×(2.6%−(−0.2%)) ="}
{"instruction": "7.55 Exclusive relationships. A survey conducted on a reasonably random sample of 203 undergraduates asked, among many other questions, about the number of exclusive relationships these students have been in. The histogram below shows the distribution of the data from this sample. The sample average is 3.2 with a standard deviation of 1.97. Number of exclusive relationships 0 2 4 6 8 10 0 20 40 60 80 100 Estimate the average number of exclusive relationships Duke students have been in using a 90% confidence interval and interpret this interval in context. Check any conditions required for inference, and note any assumptions you must make as you proceed with your calculations and conclusions. 40Project Farm on YouTube, youtu.be/xUEob2oAKVs, April 16, 2018.", "response": "Independence: it is a random sample, so we can assume that the students in this sample are in- dependent of each other with respect to number of exclusive relationships they have been in. Notice that there are no students who have had no exclu- sive relationships in the sample, which suggests some student responses are likely missing (perhaps only positive values were reported). The sample size is at least 30, and there are no particularly extreme out- liers, so the normality condition is reasonable. 90% CI: (2.97, 3.43). We are 90% confident that under- graduate students have been in 2.97 to 3.43 exclusive relationships, on average."}
{"instruction": "7.57 Online communication. A study suggests that the average college student spends 10 hours per week communicating with others online. You believe that this is an underestimate and decide to collect your own sample for a hypothesis test. You randomly sample 60 students from your dorm and find that on average they spent 13.5 hours a week communicating with others online. A friend of yours, who offers to help you with the hypothesis test, comes up with the following set of hypotheses. Indicate any errors you see. H0 : ¯x < 10 hours HA : ¯x > 13.5 hours", "response": ""}
